#!/usr/bin/env python3
"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã —É–º–Ω–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–∞
—Å–æ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —è–∑—ã–∫–∞
"""

import sys
import os
import time

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

try:
    from ai_service.services.smart_filter.demo_smart_filter import SimpleSmartFilter
    SIMPLE_FILTER_AVAILABLE = True
except ImportError:
    SIMPLE_FILTER_AVAILABLE = False


class SimpleLanguageOptimizedFilter:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —É–º–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —è–∑—ã–∫–∞"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏–ª—å—Ç—Ä–∞"""
        # –Ø–∑—ã–∫–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        self.language_patterns = {
            'ukrainian': {
                'chars': r'[—ñ—ó—î“ë]',  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —É–∫—Ä–∞–∏–Ω—Å–∫–∏–µ –±—É–∫–≤—ã
                'words': ['—Ç–æ–≤', '—ñ–Ω–Ω', '—î–¥—Ä–ø–æ—É', '–∫–∏—ó–≤', '–≤—É–ª', '–±—É–¥', '–ø–ª–∞—Ç—ñ–∂', '–ø–µ—Ä–µ–∫–∞–∑'],
                'weight': 1.2  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —É–∫—Ä–∞–∏–Ω—Å–∫–æ–≥–æ
            },
            'russian': {
                'chars': r'[—ã—ç—ä—ë]',  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ä—É—Å—Å–∫–∏–µ –±—É–∫–≤—ã  
                'words': ['–æ–æ–æ', '–∑–∞–æ', '–∏–Ω–Ω', '–æ–≥—Ä–Ω', '–º–æ—Å–∫–≤–∞', '—É–ª', '–¥–æ–º', '–ø–ª–∞—Ç–µ–∂', '–ø–µ—Ä–µ–≤–æ–¥'],
                'weight': 1.1  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
            },
            'english': {
                'chars': r'[a-z]',  # –õ–∞—Ç–∏–Ω–∏—Ü–∞
                'words': ['llc', 'inc', 'corp', 'bank', 'street', 'avenue', 'payment', 'transfer'],
                'weight': 1.0  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            }
        }
        
        # –ü—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å–∏–≥–Ω–∞–ª–æ–≤
        self.signal_patterns = {
            'names': [
                r'[–ê-–Ø–Ü–á–Ñ“ê][–∞-—è—ñ—ó—î“ë]*(?:–µ–Ω–∫–æ|–∫–æ|—Å—å–∫–∏–π|—Ü—å–∫–∏–π|—é–∫|—É–∫|–æ–≤|–µ–≤|–∏–Ω|–∏—á)',  # –°–ª–∞–≤—è–Ω—Å–∫–∏–µ —Ñ–∞–º–∏–ª–∏–∏
                r'[–ê-–Ø–Ü–á–Ñ“ê][–∞-—è—ñ—ó—î“ë]*(?:–æ–≤–∏—á|–µ–≤–∏—á|—ñ–π–æ–≤–∏—á|—ñ–≤–Ω–∞|—ó–≤–Ω–∞|–∏–Ω–∏—á|–æ–≤–Ω–∞|–µ–≤–Ω–∞)',  # –û—Ç—á–µ—Å—Ç–≤–∞
                r'[A-Z][a-z]+\s+[A-Z][a-z]+',  # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∏–º–µ–Ω–∞
            ],
            'companies': [
                r'\b(?:–¢–û–í|–ü—Ä–ê–¢|–ö–ü|–î–ü|—Ç–æ–≤–∞—Ä–∏—Å—Ç–≤–æ)\b',  # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ –û–ü–§
                r'\b(?:–û–û–û|–ó–ê–û|–û–ê–û|–ü–ê–û|–æ–±—â–µ—Å—Ç–≤–æ)\b',  # –†—É—Å—Å–∫–∏–µ –û–ü–§
                r'\b(?:LLC|Inc|Corp|Ltd|Company)\b',   # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –û–ü–§
                r'\b(?:[–ê-–Ø–Ü–á–Ñ“êA-Z][–∞-—è—ñ—ó—î“ëa-z]*[–ë–±]–∞–Ω–∫)\b',  # –ë–∞–Ω–∫–∏
            ],
            'documents': [
                r'\b(?:–Ü–ù–ù|—ñ–Ω–Ω|–Ñ–î–†–ü–û–£|—î–¥—Ä–ø–æ—É|–ú–§–û|–º—Ñ–æ)\s*\d+\b',  # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                r'\b(?:–ò–ù–ù|–∏–Ω–Ω|–û–ì–†–ù|–æ–≥—Ä–Ω|–ö–ü–ü|–∫–ø–ø|–ë–ò–ö|–±–∏–∫)\s*\d+\b',  # –†—É—Å—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                r'\b(?:TIN|EIN|SSN|SWIFT)\s*[A-Z0-9-]+\b',  # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                r'\b\d{4}-\d{2}-\d{2}\b',  # –î–∞—Ç—ã
                r'\bUA\d{2}\s*\d+\b',  # IBAN
            ]
        }
    
    def detect_language(self, text: str):
        """–ü—Ä–æ—Å—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞"""
        text_lower = text.lower()
        scores = {}
        
        for lang_name, lang_data in self.language_patterns.items():
            score = 0.0
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            if any(char in text_lower for char in lang_data['chars'] if len(lang_data['chars']) > 10):
                score += 0.5
            else:
                import re
                if re.search(lang_data['chars'], text):
                    score += 0.5
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö —Å–ª–æ–≤
            word_matches = sum(1 for word in lang_data['words'] if word in text_lower)
            if word_matches > 0:
                score += min(word_matches * 0.2, 0.5)
            
            scores[lang_name] = score * lang_data['weight']
        
        if scores:
            detected_lang = max(scores.keys(), key=lambda k: scores[k])
            weight = self.language_patterns[detected_lang]['weight']
            return detected_lang, weight, scores
        
        return 'english', 1.0, {}
    
    def analyze_text(self, text: str):
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å —è–∑—ã–∫–æ–≤–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        start_time = time.time()
        
        if not text or not text.strip():
            return {
                'decision': 'ALLOW',
                'confidence': 0.0,
                'risk': 'very_low',
                'language': 'unknown',
                'signals': {},
                'processing_time': time.time() - start_time
            }
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
        detected_language, language_weight, lang_scores = self.detect_language(text)
        
        # –î–µ—Ç–µ–∫—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤
        signals = {}
        total_confidence = 0.0
        
        for signal_type, patterns in self.signal_patterns.items():
            import re
            matches = []
            for pattern in patterns:
                found = re.findall(pattern, text, re.IGNORECASE)
                matches.extend(found)
            
            if matches:
                base_confidence = min(len(matches) * 0.3, 0.8)
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —è–∑—ã–∫–æ–≤–æ–π –±–æ–Ω—É—Å
                if signal_type in ['names', 'companies'] and detected_language in ['ukrainian', 'russian']:
                    # –ë–æ–Ω—É—Å –¥–ª—è —Å–ª–∞–≤—è–Ω—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤
                    optimized_confidence = min(base_confidence * language_weight, 1.0)
                elif signal_type == 'documents':
                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –±–æ–Ω—É—Å –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    optimized_confidence = min(base_confidence * (language_weight * 1.1), 1.0)
                else:
                    optimized_confidence = base_confidence
                
                signals[signal_type] = {
                    'confidence': optimized_confidence,
                    'matches': matches[:3],  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
                    'count': len(matches)
                }
                total_confidence += optimized_confidence * 0.33  # –†–∞–≤–Ω—ã–µ –≤–µ—Å–∞
            else:
                signals[signal_type] = {'confidence': 0.0, 'matches': [], 'count': 0}
        
        # –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è
        if total_confidence > 0.7:
            decision = 'FULL_SEARCH'
            risk = 'medium'
        elif total_confidence > 0.4:
            decision = 'MANUAL_REVIEW'
            risk = 'low'
        elif total_confidence > 0.1:
            decision = 'MANUAL_REVIEW'
            risk = 'low'
        else:
            decision = 'ALLOW'
            risk = 'very_low'
        
        return {
            'decision': decision,
            'confidence': total_confidence,
            'risk': risk,
            'language': detected_language,
            'language_weight': language_weight,
            'language_scores': lang_scores,
            'signals': signals,
            'processing_time': (time.time() - start_time) * 1000  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
        }


def run_demo():
    """–ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    print("=" * 80)
    print("üîß –£–ü–†–û–©–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –£–ú–ù–û–ì–û –§–ò–õ–¨–¢–†–ê")
    print("   –í—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ (—É–∫—Ä–∞–∏–Ω—Å–∫–∏–π, —Ä—É—Å—Å–∫–∏–π, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)")
    print("=" * 80)
    print()
    
    filter_system = SimpleLanguageOptimizedFilter()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏
    test_cases = [
        # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
        ("–ü–ª–∞—Ç—ñ–∂ –¥–ª—è –ö–æ–≤–∞–ª–µ–Ω–∫–æ –Ü–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤–∏—á –¢–û–í", "–£–∫—Ä–∞–∏–Ω—Å–∫–∏–π: –§–ò–û + –û–ü–§"),
        ("–Ü–ù–ù 1234567890 –Ñ–î–†–ü–û–£ 12345678", "–£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"),
        ("–º. –ö–∏—ó–≤ –≤—É–ª. –•—Ä–µ—â–∞—Ç–∏–∫ –±—É–¥. 1", "–£–∫—Ä–∞–∏–Ω—Å–∫–∏–π –∞–¥—Ä–µ—Å"),
        
        # –†—É—Å—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
        ("–ü–ª–∞—Ç–µ–∂ –¥–ª—è –ü–µ—Ç—Ä–æ–≤ –ò–≤–∞–Ω –°–µ—Ä–≥–µ–µ–≤–∏—á –û–û–û", "–†—É—Å—Å–∫–∏–π: –§–ò–û + –û–ü–§"),
        ("–ò–ù–ù 1234567890 –û–ì–†–ù 1234567890123", "–†—É—Å—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"),
        ("–≥. –ú–æ—Å–∫–≤–∞ —É–ª. –¢–≤–µ—Ä—Å–∫–∞—è –¥. 1", "–†—É—Å—Å–∫–∏–π –∞–¥—Ä–µ—Å"),
        
        # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
        ("Payment for John Smith LLC Company", "–ê–Ω–≥–ª–∏–π—Å–∫–∏–π: –∏–º—è + –û–ü–§"),
        ("TIN 12-3456789 EIN 12-3456789", "–ê–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"),
        ("New York 5th Avenue Bank", "–ê–Ω–≥–ª–∏–π—Å–∫–∏–π –±–∞–Ω–∫"),
        
        # –û–±—â–∏–µ —Ç–µ–∫—Å—Ç—ã
        ("–æ–ø–ª–∞—Ç–∞ –∑–∞ —Ç–æ–≤–∞—Ä", "–û–±—â–∏–π —Ç–µ—Ä–º–∏–Ω"),
        ("–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è", "–£—Å–ª—É–≥–∞"),
        ("1000 –≥—Ä–Ω", "–¢–æ–ª—å–∫–æ —Å—É–º–º–∞"),
    ]
    
    print("üìã –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
    print("-" * 80)
    print(f"{'‚Ññ':<2} {'–û–ø–∏—Å–∞–Ω–∏–µ':<25} {'–Ø–∑—ã–∫':<10} {'–†–µ—à–µ–Ω–∏–µ':<12} {'–ö–æ–Ω—Ñ–∏–¥–µ–Ω—Å':<10} {'–í—Ä–µ–º—è':<8}")
    print("-" * 80)
    
    stats = {'languages': {}, 'decisions': {}, 'total_time': 0}
    
    for i, (text, description) in enumerate(test_cases, 1):
        result = filter_system.analyze_text(text)
        
        # –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        lang = result['language']
        decision = result['decision']
        stats['languages'][lang] = stats['languages'].get(lang, 0) + 1
        stats['decisions'][decision] = stats['decisions'].get(decision, 0) + 1
        stats['total_time'] += result['processing_time']
        
        print(f"{i:2d} {description:<25} {lang:<10} {decision:<12} "
              f"{result['confidence']:.2f}{'':5} {result['processing_time']:.1f}ms")
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3 –ø—Ä–∏–º–µ—Ä–æ–≤
        if i <= 3:
            print(f"    –¢–µ–∫—Å—Ç: '{text}'")
            print(f"    –í–µ—Å —è–∑—ã–∫–∞: {result['language_weight']:.1f}")
            print(f"    –Ø–∑—ã–∫–æ–≤—ã–µ —Å–∫–æ—Ä—ã: {result['language_scores']}")
            
            detected_signals = []
            for signal_type, signal_data in result['signals'].items():
                if signal_data['count'] > 0:
                    detected_signals.append(f"{signal_type}({signal_data['count']})")
            print(f"    –°–∏–≥–Ω–∞–ª—ã: {', '.join(detected_signals) if detected_signals else '–Ω–µ—Ç'}")
            print()
    
    print("-" * 80)
    print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print()
    
    total_tests = len(test_cases)
    print(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total_tests}")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {stats['total_time']/total_tests:.1f} –º—Å")
    print()
    
    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —è–∑—ã–∫–∞–º:")
    for lang, count in stats['languages'].items():
        percentage = count / total_tests * 100
        print(f"  ‚Ä¢ {lang}: {count} ({percentage:.1f}%)")
    print()
    
    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—à–µ–Ω–∏–π:")
    for decision, count in stats['decisions'].items():
        percentage = count / total_tests * 100
        print(f"  ‚Ä¢ {decision}: {count} ({percentage:.1f}%)")
    print()
    
    # –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
    print("üéØ –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê –£–ü–†–û–©–ï–ù–ù–û–ô –°–ò–°–¢–ï–ú–´:")
    print("  ‚úÖ –ù–∏–∫–∞–∫–∏—Ö –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π")
    print("  ‚úÖ –ë—ã—Å—Ç—Ä–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º")
    print("  ‚úÖ –Ø–∑—ã–∫–æ–≤—ã–µ –±–æ–Ω—É—Å—ã –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏")
    print("  ‚úÖ –§–æ–∫—É—Å –Ω–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–º, —Ä—É—Å—Å–∫–æ–º, –∞–Ω–≥–ª–∏–π—Å–∫–æ–º")
    print("  ‚úÖ –ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
    print("  ‚úÖ –í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
    print()
    
    success_rate = (stats['decisions'].get('FULL_SEARCH', 0) + 
                   stats['decisions'].get('MANUAL_REVIEW', 0)) / total_tests * 100
    
    if success_rate >= 70:
        print("üéâ –û–¢–õ–ò–ß–ù–ê–Ø –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨!")
    elif success_rate >= 50:
        print("‚úÖ –•–û–†–û–®–ê–Ø –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨")
    else:
        print("‚ö†Ô∏è –°–ò–°–¢–ï–ú–ê –¢–†–ï–ë–£–ï–¢ –ù–ê–°–¢–†–û–ô–ö–ò")
    
    return stats


if __name__ == "__main__":
    print("–ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã...\n")
    
    results = run_demo()
    
    print("\n" + "=" * 80)
    print("‚úÖ –£–ü–†–û–©–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ!")
    print("   –ü—Ä–æ—Å—Ç–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è, –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –ø–ª–∞—Ç–µ–∂–µ–π")
    print("=" * 80)